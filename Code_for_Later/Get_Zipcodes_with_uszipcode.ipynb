{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    data = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read().split(\"#\\n\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"File {} was unable to be read.\".format(file_name))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(raw_data):\n",
    "    # The raw data comes in a giant string with tabs and newlines\n",
    "    data_split = raw_data.split(\"\\n\")\n",
    "    for i in range(len(data_split)):\n",
    "        data_split[i] = data_split[i].split(\"\\t\")\n",
    "        \n",
    "    return pd.DataFrame(data_split[2:], columns = data_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_def(param_header, regex = \"# +(\\w+) +-+ +(.+)\"):\n",
    "    params_dict = {}\n",
    "    params_type = {}\n",
    "    params = param_header.split(\"\\n\")\n",
    "    params_pattern = re.compile(regex)\n",
    "\n",
    "    for param in params:\n",
    "        a = params_pattern.search(param)\n",
    "\n",
    "        if a: \n",
    "            #print(first_t, a.group(1), a.group(2))\n",
    "            params_dict[a.group(1).lower()] = a.group(2)\n",
    "            #print(a.group(1) + \":\", a.group(2).split(',')[-1])\n",
    "\n",
    "    return params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Quality Data to get the Site Numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections: 16\n",
      "2nd section:\n",
      "# U.S. Geological Survey\n",
      "# \n",
      "# This file contains selected water-quality data for stations in the National Water Information \n",
      "# System water-quality database.  Explanation of codes found in this file are followed by\n",
      "# the retrieved data.\n",
      "\n",
      "Number of characters in actual data: 9736982\n"
     ]
    }
   ],
   "source": [
    "quality_file_name = \"LA_Water_Quality_Data.txt\"\n",
    "data = load_data(quality_file_name)\n",
    "print(\"~~~~~ Quality Data ~~~~~\")\n",
    "print(\"Number of sections:\", len(data))\n",
    "print(\"2nd section:\", data[2], sep = \"\\n\")\n",
    "print(\"Number of characters in actual data:\", len(data[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = data[13].split(\"\\n\")\n",
    "sites_pattern = re.compile(\"# +USGS (\\d+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_ids = []\n",
    "for site in sites:\n",
    "    site_found = sites_pattern.search(site)\n",
    "    if site_found:\n",
    "        site_ids.append(site_found.group(1))\n",
    "        \n",
    "len(site_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Site information to connect to Quality Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ Site Data ~~~~~\n",
      "Number of sections: 11\n",
      "Number of characters in actual data: 418205\n"
     ]
    }
   ],
   "source": [
    "site_file_name = \"LA_Water_Site_Data.txt\"\n",
    "site_data = load_data(site_file_name)\n",
    "print(\"~~~~~ Site Data ~~~~~\")\n",
    "print(\"Number of sections:\", len(site_data))\n",
    "print(\"Number of characters in actual data:\", len(site_data[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #  0:                               #  6:  \n",
    "    #  1:                               #  7: query started 2018-09-...\n",
    "    #  2: U.S. Geological Survey        #  8: there are 1212 sites...  \n",
    "    #  3: Te Site File stores...        #  9:    \n",
    "    #  4: The following selected...     # 10: Data!\n",
    "    #  5: Param_id      - parameter      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info = convert_to_df(site_data[10])\n",
    "#site_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency_cd': 'Agency',\n",
       " 'site_no': 'Site identification number',\n",
       " 'station_nm': 'Site name',\n",
       " 'site_tp_cd': 'Site type',\n",
       " 'lat_va': 'DMS latitude',\n",
       " 'long_va': 'DMS longitude',\n",
       " 'dec_lat_va': 'Decimal latitude',\n",
       " 'dec_long_va': 'Decimal longitude',\n",
       " 'coord_meth_cd': 'Latitude-longitude method',\n",
       " 'coord_acy_cd': 'Latitude-longitude accuracy',\n",
       " 'coord_datum_cd': 'Latitude-longitude datum',\n",
       " 'dec_coord_datum_cd': 'Decimal Latitude-longitude datum',\n",
       " 'district_cd': 'District code',\n",
       " 'state_cd': 'State code',\n",
       " 'county_cd': 'County code',\n",
       " 'country_cd': 'Country code',\n",
       " 'land_net_ds': 'Land net location description',\n",
       " 'map_nm': 'Name of location map',\n",
       " 'map_scale_fc': 'Scale of location map',\n",
       " 'alt_va': 'Altitude of Gage/land surface',\n",
       " 'alt_meth_cd': 'Method altitude determined',\n",
       " 'alt_acy_va': 'Altitude accuracy',\n",
       " 'alt_datum_cd': 'Altitude datum',\n",
       " 'huc_cd': 'Hydrologic unit code',\n",
       " 'basin_cd': 'Drainage basin code',\n",
       " 'topo_cd': 'Topographic setting code',\n",
       " 'data_types_cd': 'Flags for the type of data collected',\n",
       " 'instruments_cd': 'Flags for instruments at site',\n",
       " 'construction_dt': 'Date of first construction',\n",
       " 'inventory_dt': 'Date site established or inventoried',\n",
       " 'drain_area_va': 'Drainage area',\n",
       " 'contrib_drain_area_va': 'Contributing drainage area',\n",
       " 'tz_cd': 'Mean Greenwich time offset',\n",
       " 'local_time_fg': 'Local standard time flag',\n",
       " 'reliability_cd': 'Data reliability code',\n",
       " 'gw_file_cd': 'Data-other GW files',\n",
       " 'nat_aqfr_cd': 'National aquifer code',\n",
       " 'aqfr_cd': 'Local aquifer code',\n",
       " 'aqfr_type_cd': 'Local aquifer type code',\n",
       " 'well_depth_va': 'Well depth',\n",
       " 'hole_depth_va': 'Hole depth',\n",
       " 'depth_src_cd': 'Source of depth data',\n",
       " 'project_no': 'Project number',\n",
       " 'rt_bol': 'Real-time data flag',\n",
       " 'peak_begin_date': 'Peak-streamflow data begin date',\n",
       " 'peak_end_date': 'Peak-streamflow data end date',\n",
       " 'peak_count_nu': 'Peak-streamflow data count',\n",
       " 'qw_begin_date': 'Water-quality data begin date',\n",
       " 'qw_end_date': 'Water-quality data end date',\n",
       " 'qw_count_nu': 'Water-quality data count',\n",
       " 'gw_begin_date': 'Field water-level measurements begin date',\n",
       " 'gw_end_date': 'Field water-level measurements end date',\n",
       " 'gw_count_nu': 'Field water-level measurements count',\n",
       " 'sv_begin_date': 'Site-visit data begin date',\n",
       " 'sv_end_date': 'Site-visit data end date',\n",
       " 'sv_count_nu': 'Site-visit data count'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_params = get_parameter_def(site_data[5])\n",
    "site_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different Districts: {None, '36', '06'}\n",
      "Different Counties:  {None, '037'}\n",
      "Different Countries: {None, 'US'}\n",
      "Different SiteTypes: {'GW', 'LK', 'FA-WDS', 'FA-WU', 'ST', 'FA-WWTP', 'OC-CO', 'FA-SPS', 'FA-OF', 'SP', 'ES', 'GW-HZ', 'LA-SH', 'AT', None, 'GW-TH', 'SB-UZ'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Different Districts:\", set(site_info[\"district_cd\"]))\n",
    "print(\"Different Counties: \", set(site_info[\"county_cd\"]))\n",
    "print(\"Different Countries:\", set(site_info[\"country_cd\"]))\n",
    "print(\"Different SiteTypes:\", set(site_info[\"site_tp_cd\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading data for rich info zipcode database, total size 450+MB ...\n",
      "  10 MB finished ...\n",
      "  20 MB finished ...\n"
     ]
    }
   ],
   "source": [
    "from uszipcode import SearchEngine, Zipcode\n",
    "search = SearchEngine(simple_zipcode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(site_info[[\"dec_lat_va\", \"dec_long_va\"]])):\n",
    "    lat, long = site_info[[\"dec_lat_va\", \"dec_long_va\"]].iloc[i]\n",
    "    print(float(lat), float(long))\n",
    "    result = search.by_coordinates(float(lat), float(long), radius=1.0)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method by_coordinates in module uszipcode.search:\n",
      "\n",
      "by_coordinates(lat, lng, radius=25.0, zipcode_type='Standard', sort_by='dist', ascending=True, returns=5) method of uszipcode.search.SearchEngine instance\n",
      "    Search zipcode information near a coordinates on a map.\n",
      "    \n",
      "    Returns multiple results.\n",
      "    \n",
      "    :param lat: center latitude.\n",
      "    :param lng: center longitude.\n",
      "    :param radius: only returns zipcode within X miles from ``lat``, ``lng``.\n",
      "    \n",
      "    **中文文档**\n",
      "    \n",
      "    1. 计算出在中心坐标处, 每一经度和纬度分别代表多少miles.\n",
      "    2. 以给定坐标为中心, 画出一个矩形, 长宽分别为半径的2倍多一点, 找到该\n",
      "      矩形内所有的Zipcode.\n",
      "    3. 对这些Zipcode计算出他们的距离, 然后按照距离远近排序。距离超过我们\n",
      "      限定的半径的直接丢弃.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(search.by_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
